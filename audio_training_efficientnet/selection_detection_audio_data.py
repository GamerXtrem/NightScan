#!/usr/bin/env python3
"""
Script de s√©lection des donn√©es audio bas√© sur la d√©tection par mod√®le ML
Utilise un mod√®le EfficientNet entra√Æn√© pour filtrer les segments contenant de vraies d√©tections
"""

import os
import sys
import torch
import torch.nn as nn
import torchaudio
import numpy as np
from pathlib import Path
import argparse
import json
import logging
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm
import librosa
import soundfile as sf
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Ajouter le chemin parent pour les imports
sys.path.append(str(Path(__file__).parent.parent))

from models.efficientnet_config import create_audio_model, get_audio_classes
from spectrogram_config import SpectrogramConfig, get_config_for_animal

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ModelBasedDetector:
    """D√©tecteur bas√© sur un mod√®le ML pour filtrer les segments audio."""
    
    def __init__(self, 
                 model_path: str,
                 device: str = None,
                 sample_rate: int = 22050,  # Chang√© de 48000 √† 22050 (comme l'entra√Ænement)
                 n_fft: int = 2048,         # Chang√© de 1024 √† 2048 (comme l'entra√Ænement)
                 hop_length: int = 512,     # Chang√© de 320 √† 512 (comme l'entra√Ænement)
                 n_mels: int = 128,
                 fmin: int = 50,            # Chang√© de 0 √† 50 (comme l'entra√Ænement)
                 fmax: int = 11000,         # Chang√© de 24000 √† 11000 (comme l'entra√Ænement)
                 index_db: Optional[str] = None,
                 training_db: Optional[str] = None,
                 class_list_file: Optional[str] = None):
        """
        Initialise le d√©tecteur.
        
        Args:
            model_path: Chemin vers le checkpoint du mod√®le
            device: Device √† utiliser (auto-d√©tection si None)
            sample_rate: Taux d'√©chantillonnage (d√©faut: 22050 comme l'entra√Ænement)
            n_fft: Taille FFT (d√©faut: 2048 comme l'entra√Ænement)
            hop_length: Hop length pour le spectrogramme (d√©faut: 512 comme l'entra√Ænement)
            n_mels: Nombre de bandes mel (d√©faut: 128)
            fmin: Fr√©quence minimale (d√©faut: 50 comme l'entra√Ænement)
            fmax: Fr√©quence maximale (d√©faut: 11000 comme l'entra√Ænement)
            index_db: Base SQLite contenant les fichiers √† traiter (optionnel)
            training_db: Base SQLite utilis√©e pendant l'entra√Ænement (IMPORTANT: pour l'ordre des classes)
            class_list_file: Fichier texte avec les noms de classes
        """
        self.device = torch.device(device if device else ('cuda' if torch.cuda.is_available() else 'cpu'))
        self.sample_rate = sample_rate
        
        # Param√®tres spectrogramme
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels
        self.fmin = fmin
        self.fmax = fmax
        
        # Charger le mod√®le
        logger.info(f"Chargement du mod√®le depuis {model_path}")
        # PyTorch 2.6+ n√©cessite weights_only=False pour charger les checkpoints avec des objets Python
        checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)
        
        # V√©rifier si les classes sont dans le checkpoint
        self.checkpoint_class_names = checkpoint.get('class_names', None)
        if self.checkpoint_class_names:
            logger.info(f"‚úÖ Classes trouv√©es dans le checkpoint: {len(self.checkpoint_class_names)} classes")
            
        # Extraire les informations du mod√®le
        if 'args' in checkpoint:
            self.num_classes = checkpoint['args'].num_classes
            model_name = getattr(checkpoint['args'], 'model', 'efficientnet-b1')
        else:
            self.num_classes = checkpoint.get('num_classes', 10)
            model_name = checkpoint.get('model_name', 'efficientnet-b1')
        
        # Cr√©er et charger le mod√®le
        self.model = create_audio_model(
            num_classes=self.num_classes,
            model_name=model_name,
            pretrained=False,
            dropout_rate=0.0
        )
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # Obtenir les noms de classes
        self.index_db = index_db
        self.training_db = training_db
        self.class_list_file = class_list_file
        self.class_names = self._get_class_names()
        
        logger.info(f"Mod√®le charg√©: {model_name} avec {self.num_classes} classes")
        logger.info(f"Device: {self.device}")
        if len(self.class_names) <= 20:
            logger.info(f"Classes: {', '.join(self.class_names)}")
        else:
            logger.info(f"Classes: {', '.join(self.class_names[:10])} ... et {len(self.class_names)-10} autres")
        
        # Transform pour mel spectrogramme
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=self.sample_rate,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            n_mels=self.n_mels,
            f_min=self.fmin,
            f_max=self.fmax
        ).to(self.device)
        
        self.db_transform = torchaudio.transforms.AmplitudeToDB(top_db=80).to(self.device)
    
    def _get_class_names(self) -> List[str]:
        """R√©cup√®re les noms de classes depuis la base de donn√©es ou utilise les d√©fauts."""
        import sqlite3
        
        # Option 0: Si les classes sont dans le checkpoint, les utiliser (PLUS FIABLE)
        if self.checkpoint_class_names:
            if len(self.checkpoint_class_names) == self.num_classes:
                logger.info(f"‚úÖ Utilisation des classes du checkpoint (plus fiable)")
                logger.info(f"Ordre des classes (premi√®res 10): {self.checkpoint_class_names[:10]}")
                return self.checkpoint_class_names
            else:
                logger.warning(f"Classes dans checkpoint ({len(self.checkpoint_class_names)}) != num_classes ({self.num_classes})")
        
        # Option 1: Charger depuis la base d'entra√Ænement (PRIORITAIRE)
        if self.training_db and Path(self.training_db).exists():
            try:
                logger.info(f"Chargement des classes depuis la base d'entra√Ænement: {self.training_db}")
                conn = sqlite3.connect(self.training_db)
                cursor = conn.cursor()
                
                # R√©cup√©rer les classes distinctes dans le M√äME ORDRE que pendant l'entra√Ænement
                # C'est CRITIQUE: l'ordre doit √™tre identique !
                cursor.execute("""
                    SELECT DISTINCT class_name 
                    FROM audio_samples 
                    WHERE split = 'train'
                    ORDER BY class_name
                """)
                class_names = [row[0] for row in cursor.fetchall()]
                conn.close()
                
                if len(class_names) == self.num_classes:
                    logger.info(f"‚úÖ Classes charg√©es depuis la base d'entra√Ænement: {len(class_names)} classes")
                    logger.info(f"Ordre des classes (premi√®res 10): {class_names[:10]}")
                    return class_names
                else:
                    logger.error(f"‚ùå ERREUR CRITIQUE: Nombre de classes dans la base d'entra√Ænement ({len(class_names)}) != num_classes du mod√®le ({self.num_classes})")
                    logger.error(f"Classes trouv√©es: {class_names}")
                    raise ValueError(f"Incoh√©rence du nombre de classes: {len(class_names)} != {self.num_classes}")
            except Exception as e:
                logger.error(f"Erreur lecture base d'entra√Ænement: {e}")
                raise
                
        # Option 2: Charger depuis la base d'index (moins fiable)
        elif self.index_db and Path(self.index_db).exists():
            logger.warning("‚ö†Ô∏è  Utilisation de la base d'index au lieu de la base d'entra√Ænement - risque d'ordre incorrect!")
            try:
                conn = sqlite3.connect(self.index_db)
                cursor = conn.cursor()
                
                # R√©cup√©rer les classes distinctes tri√©es par ordre alphab√©tique
                cursor.execute("SELECT DISTINCT class_name FROM audio_samples ORDER BY class_name")
                class_names = [row[0] for row in cursor.fetchall()]
                conn.close()
                
                if len(class_names) == self.num_classes:
                    logger.info(f"Classes charg√©es depuis la base d'index: {len(class_names)} classes")
                    return class_names
                else:
                    logger.warning(f"Nombre de classes dans la base ({len(class_names)}) != num_classes du mod√®le ({self.num_classes})")
            except Exception as e:
                logger.error(f"Erreur lecture SQLite: {e}")
        
        # Option 3: Charger depuis un fichier texte
        elif self.class_list_file and Path(self.class_list_file).exists():
            try:
                with open(self.class_list_file, 'r') as f:
                    class_names = [line.strip() for line in f if line.strip()]
                
                if len(class_names) == self.num_classes:
                    logger.info(f"Classes charg√©es depuis {self.class_list_file}: {len(class_names)} classes")
                    return class_names
                else:
                    logger.warning(f"Nombre de classes dans le fichier ({len(class_names)}) != num_classes du mod√®le ({self.num_classes})")
            except Exception as e:
                logger.error(f"Erreur lecture fichier classes: {e}")
        
        # Option 4: Utiliser les classes par d√©faut ou g√©n√©rer
        default_classes = get_audio_classes()
        
        if self.num_classes <= len(default_classes):
            logger.error("‚ùå ATTENTION: Utilisation des classes par d√©faut - les d√©tections seront INCORRECTES!")
            logger.error("Utilisez --training-db pour sp√©cifier la base SQLite d'entra√Ænement")
            return default_classes[:self.num_classes]
        else:
            logger.error(f"‚ùå ERREUR: Impossible de d√©terminer les noms de classes pour {self.num_classes} classes")
            logger.error("Utilisez --training-db pour sp√©cifier la base SQLite d'entra√Ænement")
            raise ValueError("Impossible de charger les noms de classes - sp√©cifiez --training-db")
    
    def _generate_spectrogram(self, audio: torch.Tensor) -> torch.Tensor:
        """
        G√©n√®re un spectrogramme mel √† partir d'un signal audio.
        
        Args:
            audio: Signal audio (1, samples)
            
        Returns:
            Spectrogramme mel en dB (3, n_mels, time)
        """
        # Calculer le spectrogramme mel
        mel_spec = self.mel_transform(audio)
        
        # Convertir en dB
        mel_spec_db = self.db_transform(mel_spec)
        
        # Normaliser avec Z-score (comme pendant l'entra√Ænement)
        mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-6)
        
        # Convertir en 3 canaux pour le mod√®le
        if mel_spec_db.dim() == 3:
            mel_spec_db = mel_spec_db.squeeze(0)
        mel_spec_db = mel_spec_db.unsqueeze(0).repeat(3, 1, 1)
        
        return mel_spec_db
    
    def process_audio_window(self, audio: torch.Tensor, min_confidence: float = 0.3) -> Dict:
        """
        Process une fen√™tre audio et retourne les pr√©dictions.
        
        Args:
            audio: Signal audio (1, samples)
            min_confidence: Confiance minimale pour consid√©rer une d√©tection
            
        Returns:
            Dict avec les r√©sultats de d√©tection
        """
        with torch.no_grad():
            # G√©n√©rer le spectrogramme
            spectrogram = self._generate_spectrogram(audio)
            spectrogram = spectrogram.unsqueeze(0).to(self.device)
            
            # Pr√©diction
            outputs = self.model(spectrogram)
            probabilities = torch.softmax(outputs, dim=1)
            
            # Obtenir la classe pr√©dite et la confiance
            confidence, predicted_class = torch.max(probabilities, 1)
            confidence = confidence.item()
            predicted_class = predicted_class.item()
            
            # Obtenir les top-3 predictions
            top3_probs, top3_classes = torch.topk(probabilities, k=min(3, self.num_classes), dim=1)
            
            result = {
                'detected': confidence >= min_confidence,
                'class': self.class_names[predicted_class],
                'class_id': predicted_class,
                'confidence': confidence,
                'top3_classes': [self.class_names[idx] for idx in top3_classes[0].cpu().numpy()],
                'top3_confidences': top3_probs[0].cpu().numpy().tolist()
            }
            
            return result
    
    def process_file(self, 
                    audio_path: Path,
                    window_size: float = 3.0,
                    hop_size: float = 1.5,
                    min_confidence: float = 0.3,
                    target_classes: Optional[List[str]] = None,
                    energy_threshold: float = -50.0) -> List[Dict]:
        """
        Process un fichier audio complet avec fen√™tre glissante.
        
        Args:
            audio_path: Chemin du fichier audio
            window_size: Taille de la fen√™tre en secondes
            hop_size: D√©calage entre fen√™tres en secondes
            min_confidence: Confiance minimale
            target_classes: Liste des classes cibles (None = toutes)
            energy_threshold: Seuil d'√©nergie en dB pour pr√©-filtrage
            
        Returns:
            Liste des d√©tections
        """
        # Charger l'audio
        try:
            audio, sr = torchaudio.load(str(audio_path))
            
            # Resample si n√©cessaire
            if sr != self.sample_rate:
                resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
                audio = resampler(audio)
            
            # Convertir en mono si n√©cessaire
            if audio.shape[0] > 1:
                audio = torch.mean(audio, dim=0, keepdim=True)
            
        except Exception as e:
            logger.error(f"Erreur chargement {audio_path}: {e}")
            return []
        
        # Calculer les param√®tres de fen√™trage
        window_samples = int(window_size * self.sample_rate)
        hop_samples = int(hop_size * self.sample_rate)
        
        detections = []
        
        # Parcourir avec fen√™tre glissante
        for start_idx in range(0, audio.shape[1] - window_samples + 1, hop_samples):
            end_idx = start_idx + window_samples
            window_audio = audio[:, start_idx:end_idx]
            
            # Pr√©-filtrage par √©nergie (optionnel)
            if energy_threshold is not None:
                rms = torch.sqrt(torch.mean(window_audio**2))
                rms_db = 20 * torch.log10(rms + 1e-8)
                if rms_db < energy_threshold:
                    continue
            
            # D√©tection
            result = self.process_audio_window(window_audio, min_confidence)
            
            # Filtrer par classes cibles si sp√©cifi√©
            if target_classes and result['class'] not in target_classes:
                result['detected'] = False
            
            if result['detected']:
                result['start_time'] = start_idx / self.sample_rate
                result['end_time'] = end_idx / self.sample_rate
                result['file_path'] = str(audio_path)
                detections.append(result)
        
        return detections


def extract_and_save_segment(audio_path: Path, 
                           detection: Dict,
                           output_dir: Path,
                           sample_rate: int = 48000,
                           preserve_structure: bool = True) -> Path:
    """
    Extrait et sauvegarde un segment audio bas√© sur une d√©tection.
    
    Args:
        audio_path: Chemin du fichier audio source
        detection: Dict contenant les infos de d√©tection
        output_dir: R√©pertoire de sortie
        sample_rate: Taux d'√©chantillonnage
        preserve_structure: Pr√©server la structure des dossiers
        
    Returns:
        Chemin du fichier sauvegard√©
    """
    # Charger l'audio
    audio, sr = librosa.load(str(audio_path), sr=sample_rate, mono=True)
    
    # Extraire le segment
    start_sample = int(detection['start_time'] * sample_rate)
    end_sample = int(detection['end_time'] * sample_rate)
    segment = audio[start_sample:end_sample]
    
    # D√©terminer le chemin de sortie
    if preserve_structure:
        # Garder la structure originale des classes
        relative_path = audio_path.parent.name  # Nom de la classe
        class_output_dir = output_dir / relative_path
    else:
        # Organiser par classe d√©tect√©e
        class_output_dir = output_dir / detection['class']
    
    class_output_dir.mkdir(parents=True, exist_ok=True)
    
    # Nom du fichier avec m√©tadonn√©es
    timestamp = f"{int(detection['start_time']*1000):06d}"
    confidence = int(detection['confidence'] * 100)
    output_name = f"{audio_path.stem}_seg{timestamp}_conf{confidence}_{detection['class']}.wav"
    output_path = class_output_dir / output_name
    
    # Sauvegarder
    sf.write(str(output_path), segment, sample_rate)
    
    return output_path


def main():
    parser = argparse.ArgumentParser(
        description="S√©lection des segments audio bas√©e sur la d√©tection par mod√®le ML"
    )
    
    # Entr√©es/Sorties
    parser.add_argument('--audio-root', type=Path, required=True,
                       help="R√©pertoire racine contenant les fichiers audio")
    parser.add_argument('--output-dir', type=Path, required=True,
                       help="R√©pertoire de sortie pour les segments filtr√©s")
    
    # Mod√®le
    parser.add_argument('--model', type=str, required=True,
                       help="Chemin vers le checkpoint du mod√®le (.pth)")
    parser.add_argument('--device', type=str, default=None,
                       help="Device √† utiliser (cuda/cpu, auto si non sp√©cifi√©)")
    parser.add_argument('--index-db', type=str, default=None,
                       help="Base SQLite pour charger les noms de classes (balanced_audio_index.db)")
    parser.add_argument('--training-db', type=str, default=None,
                       help="Base SQLite utilis√©e pendant l'entra√Ænement (IMPORTANT pour l'ordre des classes)")
    parser.add_argument('--class-list', type=str, default=None,
                       help="Fichier texte contenant les noms de classes (une par ligne)")
    
    # Param√®tres de d√©tection
    parser.add_argument('--min-conf', type=float, default=0.3,
                       help="Confiance minimale pour garder un segment (d√©faut: 0.3)")
    parser.add_argument('--target-list', type=Path, default=None,
                       help="Fichier texte contenant les classes cibles (une par ligne)")
    
    # Param√®tres de fen√™trage
    parser.add_argument('--win', type=float, default=3.0,
                       help="Taille de la fen√™tre en secondes (d√©faut: 3.0 comme l'entra√Ænement)")
    parser.add_argument('--hop', type=float, default=1.5,
                       help="D√©calage entre fen√™tres en secondes (d√©faut: 1.5)")
    
    # Param√®tres audio
    parser.add_argument('--sr', type=str, default='22050',
                       help="Taux d'√©chantillonnage (d√©faut: 22050 comme l'entra√Ænement)")
    parser.add_argument('--energy-threshold', type=float, default=-50.0,
                       help="Seuil d'√©nergie en dB pour pr√©-filtrage (d√©faut: -50)")
    
    # Options
    parser.add_argument('--preserve-structure', action='store_true',
                       help="Pr√©server la structure originale des dossiers")
    parser.add_argument('--max-segments-per-file', type=int, default=None,
                       help="Nombre maximum de segments par fichier source")
    parser.add_argument('--dry-run', action='store_true',
                       help="Mode simulation - ne pas extraire les fichiers")
    parser.add_argument('--report', type=Path, default=None,
                       help="G√©n√©rer un rapport de d√©tection (JSON)")
    parser.add_argument('--verbose', action='store_true',
                       help="Afficher des informations d√©taill√©es pendant le traitement")
    parser.add_argument('--validation-mode', action='store_true',
                       help="Mode validation : compare les d√©tections avec les dossiers sources et affiche les statistiques")
    
    args = parser.parse_args()
    
    # Parser le taux d'√©chantillonnage
    if args.sr.endswith('k'):
        sample_rate = int(float(args.sr[:-1]) * 1000)
    else:
        sample_rate = int(args.sr)
    
    # Charger les classes cibles si sp√©cifi√©
    target_classes = None
    if args.target_list:
        with open(args.target_list, 'r') as f:
            target_classes = [line.strip() for line in f if line.strip()]
        logger.info(f"Classes cibles: {', '.join(target_classes)}")
    
    # Avertissement si pas de base d'entra√Ænement
    if not args.training_db:
        logger.warning("‚ö†Ô∏è  ATTENTION: --training-db non sp√©cifi√©")
        logger.warning("Les classes pourraient √™tre dans le mauvais ordre, causant des d√©tections incorrectes")
        logger.warning("Utilisez la m√™me base SQLite que celle utilis√©e pour l'entra√Ænement")
        
    # Cr√©er le d√©tecteur
    detector = ModelBasedDetector(
        model_path=args.model,
        device=args.device,
        sample_rate=sample_rate,
        index_db=args.index_db,
        training_db=args.training_db,
        class_list_file=args.class_list
    )
    
    # Scanner les fichiers audio
    audio_extensions = ['.wav', '.mp3', '.flac', '.ogg']
    audio_files = []
    
    for ext in audio_extensions:
        files = args.audio_root.rglob(f'*{ext}')
        # Filtrer les fichiers cach√©s macOS (commen√ßant par ._)
        audio_files.extend([f for f in files if not f.name.startswith('._')])
    
    logger.info(f"Fichiers audio trouv√©s: {len(audio_files)}")
    
    # Cr√©er le r√©pertoire de sortie
    if not args.dry_run:
        args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # Statistiques globales
    global_stats = {
        'timestamp': datetime.now().isoformat(),
        'model_path': str(args.model),
        'min_confidence': args.min_conf,
        'window_size': args.win,
        'hop_size': args.hop,
        'total_files': len(audio_files),
        'total_detections': 0,
        'detections_by_class': {},
        'files_with_detections': 0,
        'all_detections': [],
        'validation_stats': {
            'correct_detections': 0,
            'incorrect_detections': 0,
            'accuracy_by_folder': {},
            'confusion_matrix': {}
        } if args.validation_mode else None
    }
    
    # Traiter chaque fichier
    pbar = tqdm(audio_files, desc="Traitement des fichiers")
    for file_idx, audio_file in enumerate(pbar):
        # D√©tections pour ce fichier
        detections = detector.process_file(
            audio_file,
            window_size=args.win,
            hop_size=args.hop,
            min_confidence=args.min_conf,
            target_classes=target_classes,
            energy_threshold=args.energy_threshold
        )
        
        # Classe originale du fichier (nom du dossier)
        original_class = audio_file.parent.name
        
        # Statistiques de validation si mode validation
        if args.validation_mode and detections:
            if original_class not in global_stats['validation_stats']['accuracy_by_folder']:
                global_stats['validation_stats']['accuracy_by_folder'][original_class] = {
                    'correct': 0, 'incorrect': 0, 'total': 0
                }
            
            # Initialiser la matrice de confusion pour cette classe
            if original_class not in global_stats['validation_stats']['confusion_matrix']:
                global_stats['validation_stats']['confusion_matrix'][original_class] = {}
        
        # Logs d√©taill√©s si verbose
        if args.verbose and detections:
            logger.info(f"\n{'='*60}")
            logger.info(f"Fichier: {audio_file.name}")
            logger.info(f"Classe originale: {original_class}")
            logger.info(f"D√©tections trouv√©es: {len(detections)}")
            
            # Afficher les d√©tections avec validation
            for i, det in enumerate(detections[:5]):  # Limiter √† 5 pour ne pas spam
                is_correct = det['class'] == original_class
                status = "‚úì" if is_correct else "‚úó"
                logger.info(f"  [{i+1}] {det['start_time']:.1f}-{det['end_time']:.1f}s : "
                          f"{det['class']} (conf: {det['confidence']:.2%}) {status}")
            if len(detections) > 5:
                logger.info(f"  ... et {len(detections)-5} autres d√©tections")
        
        if detections:
            global_stats['files_with_detections'] += 1
            
            # Limiter le nombre de segments par fichier si demand√©
            if args.max_segments_per_file and len(detections) > args.max_segments_per_file:
                # Garder les d√©tections avec la plus haute confiance
                detections.sort(key=lambda x: x['confidence'], reverse=True)
                detections = detections[:args.max_segments_per_file]
                if args.verbose:
                    logger.info(f"  ‚Üí Limit√© √† {args.max_segments_per_file} segments (plus haute confiance)")
            
            # Extraire et sauvegarder les segments
            for detection in detections:
                # Validation : v√©rifier si la d√©tection correspond au dossier
                is_correct_detection = detection['class'] == original_class
                
                # En mode validation, compter et √©ventuellement filtrer
                if args.validation_mode:
                    global_stats['validation_stats']['accuracy_by_folder'][original_class]['total'] += 1
                    
                    if is_correct_detection:
                        global_stats['validation_stats']['correct_detections'] += 1
                        global_stats['validation_stats']['accuracy_by_folder'][original_class]['correct'] += 1
                    else:
                        global_stats['validation_stats']['incorrect_detections'] += 1
                        global_stats['validation_stats']['accuracy_by_folder'][original_class]['incorrect'] += 1
                        
                        # Matrice de confusion
                        detected_class = detection['class']
                        if detected_class not in global_stats['validation_stats']['confusion_matrix'][original_class]:
                            global_stats['validation_stats']['confusion_matrix'][original_class][detected_class] = 0
                        global_stats['validation_stats']['confusion_matrix'][original_class][detected_class] += 1
                    
                    # En mode validation strict, ne garder que les d√©tections correctes
                    if not is_correct_detection and not args.dry_run:
                        continue  # Skip cette d√©tection
                
                global_stats['total_detections'] += 1
                
                # Statistiques par classe
                class_name = detection['class']
                if class_name not in global_stats['detections_by_class']:
                    global_stats['detections_by_class'][class_name] = 0
                global_stats['detections_by_class'][class_name] += 1
                
                # Sauvegarder si pas en mode dry-run
                if not args.dry_run:
                    output_path = extract_and_save_segment(
                        audio_file,
                        detection,
                        args.output_dir,
                        sample_rate,
                        args.preserve_structure
                    )
                    detection['output_path'] = str(output_path)
                elif args.verbose:
                    # En dry-run verbose, montrer ce qui serait sauv√©
                    status = "‚úì" if is_correct_detection else "‚úó"
                    logger.info(f"  [DRY-RUN] Sauverait: {detection['start_time']:.1f}-{detection['end_time']:.1f}s "
                              f"comme {detection['class']}_conf{int(detection['confidence']*100)}.wav {status}")
                
                # Ajouter aux d√©tections globales pour le rapport
                if args.report:
                    detection['is_correct'] = is_correct_detection
                    detection['original_class'] = original_class
                    global_stats['all_detections'].append(detection)
        
        # Mettre √† jour la barre de progression avec les stats
        if args.validation_mode and global_stats['validation_stats']['correct_detections'] + global_stats['validation_stats']['incorrect_detections'] > 0:
            accuracy = global_stats['validation_stats']['correct_detections'] / (global_stats['validation_stats']['correct_detections'] + global_stats['validation_stats']['incorrect_detections']) * 100
            pbar.set_postfix({
                'D√©tections': global_stats['total_detections'],
                'Corrects': global_stats['validation_stats']['correct_detections'],
                'Pr√©cision': f"{accuracy:.1f}%",
                'Classe': original_class[:15]
            })
        else:
            pbar.set_postfix({
                'D√©tections': global_stats['total_detections'],
                'Fichiers OK': global_stats['files_with_detections'],
                'Classe': original_class[:15]
            })
        
        # R√©sum√© p√©riodique
        if (file_idx + 1) % 100 == 0:
            logger.info(f"\n--- R√©sum√© apr√®s {file_idx + 1} fichiers ---")
            logger.info(f"Fichiers avec d√©tections: {global_stats['files_with_detections']}")
            logger.info(f"Total d√©tections: {global_stats['total_detections']}")
            logger.info(f"D√©tections par classe: {dict(sorted(global_stats['detections_by_class'].items()))}")
    
    # Afficher les r√©sultats
    print(f"\n{'='*60}")
    print("R√âSULTATS DE LA S√âLECTION PAR D√âTECTION")
    print(f"{'='*60}")
    print(f"Fichiers trait√©s: {global_stats['total_files']}")
    print(f"Fichiers avec d√©tections: {global_stats['files_with_detections']}")
    print(f"Total d√©tections: {global_stats['total_detections']}")
    
    # R√©sultats de validation si mode validation
    if args.validation_mode:
        val_stats = global_stats['validation_stats']
        total_val = val_stats['correct_detections'] + val_stats['incorrect_detections']
        if total_val > 0:
            accuracy = val_stats['correct_detections'] / total_val * 100
            print(f"\nüéØ STATISTIQUES DE VALIDATION:")
            print(f"  D√©tections correctes: {val_stats['correct_detections']}")
            print(f"  D√©tections incorrectes: {val_stats['incorrect_detections']}")
            print(f"  Pr√©cision globale: {accuracy:.2f}%")
            
            print(f"\nüìä Pr√©cision par dossier:")
            for folder, stats in sorted(val_stats['accuracy_by_folder'].items()):
                if stats['total'] > 0:
                    folder_accuracy = stats['correct'] / stats['total'] * 100
                    print(f"  {folder}: {stats['correct']}/{stats['total']} ({folder_accuracy:.1f}%)")
            
            # Top confusions
            print(f"\n‚ùå Top 10 confusions:")
            confusions = []
            for true_class, predictions in val_stats['confusion_matrix'].items():
                for pred_class, count in predictions.items():
                    confusions.append((true_class, pred_class, count))
            confusions.sort(key=lambda x: x[2], reverse=True)
            for true_class, pred_class, count in confusions[:10]:
                print(f"  {true_class} ‚Üí {pred_class}: {count}")
    
    print(f"\nD√©tections par classe:")
    for class_name, count in sorted(global_stats['detections_by_class'].items()):
        print(f"  {class_name}: {count}")
    
    # Sauvegarder le rapport si demand√©
    if args.report:
        # Limiter la taille du rapport
        if len(global_stats['all_detections']) > 1000:
            logger.warning("Rapport limit√© aux 1000 premi√®res d√©tections")
            global_stats['all_detections'] = global_stats['all_detections'][:1000]
        
        with open(args.report, 'w') as f:
            json.dump(global_stats, f, indent=2)
        print(f"\nRapport sauvegard√©: {args.report}")
    
    if args.dry_run:
        print("\n‚ö†Ô∏è  Mode DRY-RUN - Aucun fichier n'a √©t√© cr√©√©")
    else:
        print(f"\n‚úÖ Segments sauvegard√©s dans: {args.output_dir}")


if __name__ == "__main__":
    main()